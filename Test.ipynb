{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.3681,  1.0725,  1.1745,  ...,  0.6649,  1.8754, -1.1738],\n        [ 1.9555,  3.0755, -0.1360,  ...,  0.4308, -0.4491, -0.5433],\n        [-0.6081, -0.5714,  1.8643,  ..., -1.3304, -0.9567,  1.0136],\n        ...,\n        [-1.1398,  0.8878,  0.5903,  ..., -0.1679, -2.0859, -0.3872],\n        [-2.0747, -0.8561,  0.4827,  ...,  0.2829, -2.4995,  1.3371],\n        [-0.6228,  0.0829, -0.0374,  ...,  0.6859, -0.2517,  0.3112]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(32, 64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 64\n",
    "LR_G = 0.0001           # learning rate for generator\n",
    "LR_D = 0.0001           # learning rate for discriminator\n",
    "N_IDEAS = 5             # think of this as number of ideas for generating an art work (Generator)\n",
    "ART_COMPONENTS = 15     # it could be total point G can draw in the canvas\n",
    "PAINT_POINTS = np.vstack([np.linspace(-1, 1, ART_COMPONENTS) for _ in range(BATCH_SIZE)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ],\n       [-1.        , -0.85714286, -0.71428571, -0.57142857, -0.42857143,\n        -0.28571429, -0.14285714,  0.        ,  0.14285714,  0.28571429,\n         0.42857143,  0.57142857,  0.71428571,  0.85714286,  1.        ]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAINT_POINTS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(shape=(10,2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros_like(np.arange(20).reshape(5,4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "Epoch [0/2] Batch 0/1875                               Loss D: 0.6079, loss G: 0.6958\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n",
      "fake.shape:torch.Size([32, 784])\n",
      "disc_real.shape:torch.Size([32])\n",
      "disc_fake.shape:torch.Size([32])\n",
      "noise.shape:torch.Size([32, 64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/zx/xctpf9r50q9628v05ll3x86r0000gp/T/ipykernel_4076/3165336129.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     77\u001B[0m         \u001B[0mnoise\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mz_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'noise.shape:{noise.shape}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 79\u001B[0;31m         \u001B[0mfake\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgenerator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnoise\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     80\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'fake.shape:{fake.shape}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     81\u001B[0m         \u001B[0mdisc_real\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdiscriminator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimgs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/zx/xctpf9r50q9628v05ll3x86r0000gp/T/ipykernel_4076/3165336129.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 103\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mlinear\u001B[0;34m(input, weight, bias)\u001B[0m\n\u001B[1;32m   1846\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mhas_torch_function_variadic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1847\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mhandle_torch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1848\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1849\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1850\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Things to try\n",
    "# 1.What happens if you use larger network?\n",
    "# 2.Better normalization with BatchNorm\n",
    "# 3.Different learning rate?\n",
    "# 4.Change architecture to CNN?\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate = 3e-4\n",
    "z_dim = 64\n",
    "img_dim = 28 * 28 * 1\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0\n",
    "\n",
    "discriminator = Discriminator(in_features=img_dim).to(device)\n",
    "generator = Generator(z_dim=z_dim, img_dim=img_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose([\n",
    "    # torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "    torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "# 上面的ToTensor()可以完成归一化，同时Normalize把mean和std设置为0.5,可以把所有[0,1]的值投影到[-1,1]的区间内\n",
    "# 这也是为什么最后的Generator要加一层tanh，是因为tanh会把所有的值投射到[-1,1]\n",
    "\n",
    "datasets = datasets.MNIST(root='./MNIST_dataset', transform=transforms, download=True)\n",
    "dataloader = DataLoader(dataset=datasets, batch_size=batch_size, shuffle=True)\n",
    "optimizer_disc = torch.optim.Adam(params=discriminator.parameters(), lr=learning_rate)\n",
    "optimizer_gen = torch.optim.Adam(params=generator.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (imgs, _) in enumerate(dataloader):\n",
    "        imgs = imgs.reshape(-1, 784).to(device)\n",
    "        batch_size = imgs.shape[0]   # 这一行与前面的batch_size是一个意思\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        # 但通过查看BCELoss的Pytorch官方文档，可以看到其表达式前面存在一个负号，因而对于lossD来说是要Minimize loss\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        print(f'noise.shape:{noise.shape}')   # Size([32, 64])\n",
    "        fake = generator(noise)\n",
    "        print(f'fake.shape:{fake.shape}')    # Size([32, 784])\n",
    "        disc_real = discriminator(imgs).reshape(-1)\n",
    "        print(f'disc_real.shape:{disc_real.shape}')    # 最后返回的都是一个数，同时最后是在一个batch_size下，所以是batch_size个数\n",
    "        lossD_real = loss_fn(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = discriminator(fake).reshape(-1)\n",
    "        print(f'disc_fake.shape:{disc_fake.shape}')   # 最后返回的都是一个数，同时最后是在一个batch_size下，所以是batch_size个数\n",
    "        # 这里同理，是因为discriminator和generator都是返回的一个数字\n",
    "        lossD_fake = loss_fn(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        optimizer_disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)  # retain the grads and the grads will not be freed\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = discriminator(fake).reshape(-1)\n",
    "        lossG = loss_fn(output, torch.ones_like(output))\n",
    "\n",
    "        optimizer_gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                              Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = imgs.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['False',\n 'None',\n 'True',\n '__peg_parser__',\n 'and',\n 'as',\n 'assert',\n 'async',\n 'await',\n 'break',\n 'class',\n 'continue',\n 'def',\n 'del',\n 'elif',\n 'else',\n 'except',\n 'finally',\n 'for',\n 'from',\n 'global',\n 'if',\n 'import',\n 'in',\n 'is',\n 'lambda',\n 'nonlocal',\n 'not',\n 'or',\n 'pass',\n 'raise',\n 'return',\n 'try',\n 'while',\n 'with',\n 'yield']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keyword\n",
    "keyword.kwlist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(2, 5)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(10).reshape(2,5)\n",
    "a.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(2, 5)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(a).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 5)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares =np.array([[1,4,9,16,25]])\n",
    "squares.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(5,)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(squares).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[7, 7],\n       [7, 7]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.full((2,2),7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.71978008, 0.33602709])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.73465135, 0.83785186],\n       [0.19787237, 0.02900143]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.8523121 , 0.55072865],\n       [0.28225761, 0.38511207]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((2,2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 9 6 4 8 1]\n",
      " [7 1 2 6 3 9]\n",
      " [3 1 9 7 6 9]\n",
      " [0 5 9 6 8 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[4, 1, 4, 6, 8, 9],\n       [1, 2, 3, 6, 7, 9],\n       [1, 3, 6, 7, 9, 9],\n       [0, 0, 5, 6, 8, 9]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(0,10,(4,6))\n",
    "print(x)\n",
    "np.partition(x, 3, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mglearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/zx/xctpf9r50q9628v05ll3x86r0000gp/T/ipykernel_4076/2899697075.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# generate dataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmglearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdatasets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake_forge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;31m# plot dataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mmglearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdiscrete_scatter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlegend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Class 0\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Class 1\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mglearn' is not defined"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}